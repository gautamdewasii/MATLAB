% Solve an Input-Output Fitting problem with a Neural Network
% Script generated by Neural Fitting app
% Created 17-Jan-2024 09:30:00
%
% This script assumes these variables are defined:
%
% cleaning the workspace and other environment before training model
clc;
clear;
close all;

% Loading dataset
% NOTE :- This dataset contains two input features and two output features
dataset=load('engine_dataset')
%   x - input data.
%   y1 - target data.
% Creating data for model training and testing
x=dataset.engineInputs;
y1=dataset.engineTargets;
% % right now, just predicted on one target feature
% y1=dataset.engineTargets(1,:)

% to plot these x and y values on 2d graph
% plot() used to generate 2d plots
% plot(X,Y,'LineSpecifications')
% X,Y and vectores of same length(or elements)
% 'linespec' to customize the appearence of plot
% ex :- 'r','b','g' for red,blue and green or can use ASCII values
% 'o','x','+' for marker symbols
% '-',':' , '-.' are for solid,dotted and dash dot line
% plot(x,y)
% plot(x,y,'r--o')   % r for red, - for dotted line , -o for circle marker


% 'x' is input features
% 't' is target features
x;
t = y1;

% Choose a Training Function
% For a list of all training functions type: help nntrain
% 'trainlm' is usually fastest.
% 'trainbr' takes longer but may be better for challenging problems.
% 'trainscg' uses less memory. Suitable in low memory situations.
trainFcn = 'trainlm';  % Levenberg-Marquardt backpropagation.

% Create a Fitting Network
% number of neurons in hidden layers
% 2 layers with 3 and 4 neurons
hiddenLayerSize = [3 4];

% 'net' is the network fitting , having 8 neurons and trainml algorithms in
%net = fitnet(hiddenLayerSize,trainFcn);
% using newff() method, used to munually gives the activation functions
TF={'logsig','tansig','purelin'}; % activation functions
net=newff(x,t,hiddenLayerSize,TF);


% Choose Input and Output Pre/Post-Processing Functions
% For a list of all processing functions type: help nnprocess
% it will remove all doublicate and contatnt rows ( like each feature contain same value in it )
% 'mapminmax' is used to scale these input features and output feature values into -1 to +1 range
net.input.processFcns = {'removeconstantrows','mapminmax'};
net.output.processFcns = {'removeconstantrows','mapminmax'};

% Setup Division of Data for Training, Validation, Testing
% For a list of all data division functions type: help nndivision
net.divideFcn = 'dividerand';  % Divide data randomly
% 'sample' is for static network
% 'time' is for dynamic nrtwork
% 'sampletime' is for both 
% 'all' to divide up target by every scaler value
% 'none' no divide up data at all, mean all data is for training
 net.divideMode = 'sample';  % Divide up every sample

net.divideParam.trainRatio = 70/100;
net.divideParam.valRatio = 15/100;
net.divideParam.testRatio = 15/100;

% Choose a Performance Function
% For a list of all performance functions type: help nnperformance
net.performFcn = 'mse';  % Mean Squared Error

% changing all the Training parameters plots :- 
% if true show performance window
% net.trainParam.showWindow=true;

% if true, show all details at command windows
% net.trainParam.showCommandLine=true;
% at every 10th epochs shpws details
% net.trainParam.show=10;
% number of epochs used 
% net.trainParam.epochs=1000;
% net.trainParam.goal=1e-3;


% Choose Plot Functions
% For a list of all plot functions type: help nnplot
net.plotFcns = {'plotperform','plottrainstate','ploterrhist', ...
    'plotregression', 'plotfit'};

% Train the Network ( passing input, target and network to it )
% 'tr' contains information about training process
[net,tr] = train(net,x,t);

% Test the Network
% predicted output feature 'y'
y = net(x);
% error = target - predicted
e = gsubtract(t,y);
% return the performance of the model
performance = perform(net,t,y);

% Recalculate Training, Validation and Test Performance
% examples :- isnan(tr.testMask{1}) , return 1 where value is Nan
% examples :- ~isnan(tr.testMask{1}) , return 1 where value is not Nan
% examples :- find(~isnan(tr.testMask{1})) , return indexes where value is not Nan

%% tr.trainMask{1} replace all training data with '1' and others with 'Nan'
% t.*tr.train{1} it will return actual values of training data varables
% trainTargets = t .* tr.trainMask{1};
% return all data selected for validation 
% valTargets = t .* tr.valMask{1};
% data selected for testing
% testTargets = t .* tr.testMask{1};
% % individual performance for training, testing and validation stage
% trainPerformance = perform(net,trainTargets,y);
% valPerformance = perform(net,valTargets,y);
% testPerformance = perform(net,testTargets,y);
%%

% Training , validation and testing performance code modified :-
% training 
trainIndx=find(~isnan(tr.trainMask{1}(1,:)));
trainInputs=x(:,trainIndx);
trainTargets=t(:,trainIndx);
trainOutput=y(:,trainIndx);
trainError=trainTargets-trainOutput;
% validation
valIndx=find(~isnan(tr.valMask{1}(1,:)));
valInputs=x(:,valIndx);
valTargets=t(:,valIndx);
valOutput=y(:,valIndx);
valError=valTargets-valOutput;
% testing
testIndx=find(~isnan(tr.testMask{1}(1,:)));
testInputs=x(:,testIndx);
testTargets=t(:,testIndx);
testOutput=y(:,testIndx);
testError=testTargets-testOutput;



% % View the Network
% view(net)
% Plots for #1 target
displayResult(t(1,:),y(1,:),'All Data #1');
displayResult(trainTargets(1,:),trainOutput(1,:),'Train Data #1');
displayResult(valTargets(1,:),valOutput(1,:),'Validation Data #1');
displayResult(testTargets(1,:),testOutput(1,:),'Test Data #1');

% Plots for #2 target
displayResult(t(2,:),y(2,:),'All Data');
displayResult(trainTargets(2,:),trainOutput(2,:),'Train Data #2');
displayResult(valTargets(2,:),valOutput(2,:),'Validation Data #2');
displayResult(testTargets(2,:),testOutput(2,:),'Test Data #2');



% Deployment
% Change the (false) values to (true) to enable the following code blocks.
% See the help for each generation function for more information.
if (false)
    % Generate MATLAB function for neural network for application
    % deployment in MATLAB scripts or with MATLAB Compiler and Builder
    % tools, or simply to examine the calculations your trained neural
    % network performs.
    genFunction(net,'myNeuralNetworkFunction');
    y = myNeuralNetworkFunction(x);
end
if (false)
    % Generate a matrix-only MATLAB function for neural network code
    % generation with MATLAB Coder tools.
    genFunction(net,'myNeuralNetworkFunction','MatrixOnly','yes');
    y = myNeuralNetworkFunction(x);
end
if (false)
    % Generate a Simulink diagram for simulation or deployment with.
    % Simulink Coder tools.
    gensim(net);
end
